{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.functional import softmax\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deeplabv3plus_path = r'E:\\codes\\python\\area51m\\pytorch_deeplab_xception'\n",
    "Data_path = r'D:\\Documents\\GitHub\\plant-segmentation'\n",
    "\n",
    "TrainingLabelNames = 'TrainingLabelNames.csv'\n",
    "TestLabelNames = 'TestLabelNames.csv'\n",
    "Training_input_folder = 'input'\n",
    "Training_label_folder = 'download'\n",
    "Test_input_folder = 'input_test'\n",
    "Test_label_folder = 'download_test'\n",
    "\n",
    "CroppedTrainingLabelNames = 'cropped_lb.csv'\n",
    "CroppedTestLabelNames = 'cropped_lb_test.csv'\n",
    "CroppedTraining_input_folder = 'cropped_input'\n",
    "CroppedTraining_label_folder = 'cropped_download'\n",
    "CroppedTest_input_folder = 'cropped_input_test'\n",
    "CroppedTest_label_folder = 'cropped_download_test'\n",
    "\n",
    "ModelPath = r'E:\\data\\MODELS' \n",
    "\n",
    "OneStageModelName = 'model_deeplabv3+_resnet_onestage'\n",
    "TwoStageModelName = 'model_deeplabv3+_resnet_twostage'\n",
    "\n",
    "SfMPath = r'E:\\data\\testagain'\n",
    "\n",
    "SfMSparseModel = 'sparsemodel.nvm'\n",
    "SfMTextureFolder = 'textures'\n",
    "OriginalImageFolder = 'jpgimages'\n",
    "OutputFolder = 'superimposed_window'\n",
    "\n",
    "OpenMVSPath = r'C:\\Users\\assembled\\OpenMVS\\src\\bin\\vc15\\x64\\Release'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import sys\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "sys.path.append(Deeplabv3plus_path)\n",
    "from pytorch_deeplab_xception.modeling import deeplab\n",
    "\n",
    "os.environ['TORCH_HOME'] = ModelPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (900,900)\n",
    "image_size_detail = (200,200)\n",
    "number_of_class_firstmodel = 8\n",
    "number_of_class_secondmodel = 2\n",
    "\n",
    "transform_deeplab = transforms.Compose([transforms.Resize(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
    "\n",
    "transform_deeplab_detail = transforms.Compose([transforms.Resize(image_size_detail),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
    "\n",
    "dlab2 = deeplab.DeepLab(num_classes=number_of_class_firstmodel,backbone = 'resnet').to(device)\n",
    "\n",
    "dlab2.load_state_dict(torch.load(r'{0}\\{1}_last.pt'.format(OneStageModelPath,OneStageModelName)))\n",
    "\n",
    "_ = dlab2.eval()\n",
    "\n",
    "for p in dlab2.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "dlab2_detail = deeplab.DeepLab(num_classes=number_of_class_secondmodel,backbone = 'resnet').to(device)\n",
    "\n",
    "dlab2_detail.load_state_dict(torch.load(r'{0}\\{1}_last.pt'.format(TwoStageModelPath,TwoStageModelName)))\n",
    "\n",
    "for p in dlab2_detail.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "_ = dlab2_detail.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = (255,0,0)\n",
    "green = (0,255,0)\n",
    "blue = (0,0,255)\n",
    "yellow = (255,225,0)\n",
    "orange = (255,125,0)\n",
    "purple = (155,0,255)\n",
    "greenblue = (0,255,225)\n",
    "pink = (255,140,248)\n",
    "white = (255,255,255)\n",
    "black = (0,0,0)\n",
    "colors = [red,green,blue,yellow,orange,purple,greenblue,pink]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here a NVM needs to be created by VisualSfM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## masking original images with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = os.listdir(r'{0}\\{1}'.format(SfMPath,OriginalImageFolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0\n",
      "finished 1\n",
      "finished 2\n",
      "finished 3\n",
      "finished 4\n",
      "finished 5\n",
      "finished 6\n",
      "finished 7\n",
      "finished 8\n",
      "finished 9\n",
      "finished 10\n",
      "finished 11\n",
      "finished 12\n",
      "finished 13\n",
      "finished 14\n",
      "finished 15\n",
      "finished 16\n",
      "finished 17\n",
      "finished 18\n",
      "finished 19\n",
      "finished 20\n",
      "finished 21\n",
      "finished 22\n",
      "finished 23\n",
      "finished 24\n",
      "finished 25\n",
      "finished 26\n",
      "finished 27\n",
      "finished 28\n",
      "finished 29\n",
      "finished 30\n",
      "finished 31\n",
      "finished 32\n",
      "finished 33\n",
      "finished 34\n",
      "finished 35\n",
      "finished 36\n",
      "finished 37\n",
      "finished 38\n",
      "finished 39\n",
      "finished 40\n",
      "finished 41\n",
      "finished 42\n",
      "finished 43\n",
      "finished 44\n",
      "finished 45\n",
      "finished 46\n",
      "finished 47\n",
      "finished 48\n",
      "finished 49\n",
      "finished 50\n",
      "finished 51\n",
      "finished 52\n",
      "finished 53\n",
      "finished 54\n",
      "finished 55\n",
      "finished 56\n",
      "finished 57\n",
      "finished 58\n",
      "finished 59\n",
      "finished 60\n",
      "finished 61\n",
      "finished 62\n",
      "finished 63\n",
      "finished 64\n",
      "finished 65\n",
      "finished 66\n",
      "finished 67\n",
      "finished 68\n",
      "finished 69\n",
      "finished 70\n",
      "finished 71\n",
      "finished 72\n",
      "finished 73\n",
      "finished 74\n",
      "finished 75\n",
      "finished 76\n",
      "finished 77\n",
      "finished 78\n",
      "finished 79\n",
      "finished 80\n",
      "finished 81\n",
      "finished 82\n",
      "finished 83\n",
      "finished 84\n",
      "finished 85\n",
      "finished 86\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(names)):\n",
    "    ori_img = Image.open(r'{0}\\{1}\\{2}'.format(SfMPath,OriginalImageFolder,i))\n",
    "    try:\n",
    "        if two_stage:\n",
    "            predicted_img = predict(ori_img,transform_deeplab,transform_deeplab_detail,dlab2,dlab2_detail,3,ori_img.size,scale = 0.05)\n",
    "        else:\n",
    "            predicted_img = predict_by_class(ori_img,transform_deeplab,dlab2,'all',colors)\n",
    "        overlay(image = ori_img,mask = predicted_img,alpha = 1,\n",
    "                color = white,color_bg = black,alpha_bg = 1).save('{0}\\{1}\\{2}.jpg'.format(SfMPath,OutputFolder,i))\n",
    "        print('finished {0}'.format(i))\n",
    "    except:\n",
    "        print('error {0}'.format(i))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(OpenMVSPath)\n",
    "\n",
    "os.popen(r'InterfaceVisualSFM -w {0} {1}'.format(SfMPath,SfMSparseModel)).read()\n",
    "\n",
    "os.popen(r'ReconstructMesh -w {0} {1}.mvs'.format(SfMPath,SfMSparseModel.split('.')[0])).read()\n",
    "\n",
    "for i in os.listdir(output_path):\n",
    "    shutil.move(r'{0}\\{1}\\{2}'.format(SfMPath,OutputFolder,i),r'{0}\\{1}\\{2}'.format(SfMPath,SfMTextureFolder,i))\n",
    "    \n",
    "os.popen(r'TextureMesh -w {0} {1}.mvs'.format(SfMPath,SfMSparseModel.split('.')[0])).read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
